{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating HTML index of packages in '.Library'\n",
      "Making 'packages.html' ... done\n",
      "Updating HTML index of packages in '.Library'\n",
      "Making 'packages.html' ... done\n",
      "Loading required package: MASS\n",
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n"
     ]
    }
   ],
   "source": [
    "# installs the packages if necessary and then uses them\n",
    "install.packages('klaR', repos='http://cran.us.r-project.org', dependencies=TRUE)\n",
    "install.packages('caret', repos='http://cran.us.r-project.org', dependencies=TRUE)\n",
    "library('klaR')\n",
    "library('caret')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"3.1\"\n",
      "[1] \"PART A:\"\n",
      "[1] \"Training score:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.759674796747967"
      ],
      "text/latex": [
       "0.759674796747967"
      ],
      "text/markdown": [
       "0.759674796747967"
      ],
      "text/plain": [
       "[1] 0.7596748"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Test score:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.773856209150327"
      ],
      "text/latex": [
       "0.773856209150327"
      ],
      "text/markdown": [
       "0.773856209150327"
      ],
      "text/plain": [
       "[1] 0.7738562"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('3.1')\n",
    "print('PART A:')\n",
    "\n",
    "# reads in all the data\n",
    "all_data<-read.csv('pima-indians-diabetes.data', header=FALSE)\n",
    "\n",
    "feature_vector<-all_data[,-c(9)]\n",
    "class_vector<-all_data[,9]\n",
    "train_score<-array(dim=10)\n",
    "test_score<-array(dim=10)\n",
    "for (iteration in 1:10)\n",
    "{\n",
    "    # splits the data into a partition using 80% of the data\n",
    "    subset_data<-createDataPartition(y=class_vector, p=.8, list=FALSE)\n",
    "\n",
    "    # filters subset_data to get the 80% of the rows that were present in subset_data\n",
    "    # the filter is provided by the feature_vectors variable    \n",
    "    subset_feature_vector<-feature_vector[subset_data, ]\n",
    "    \n",
    "    # this essentially gets the 9th column from the subset_data    \n",
    "    subset_class_vector<-class_vector[subset_data]\n",
    "    \n",
    "    # positive training set - I define it as the training set whose classification is equal to 1\n",
    "    # this is another filtering process\n",
    "    pos_set_train<-subset_feature_vector[subset_class_vector > 0, ]\n",
    "    \n",
    "    # negative training set - pretty much the opposite as above\n",
    "    # this is another filtering process, but does not derive from pos_set\n",
    "    neg_set_train<-subset_feature_vector[!(subset_class_vector > 0),]\n",
    "\n",
    "    # this is the log of the prior that we are going to add in (part of Naive Bayes)\n",
    "    prob_add_log<-log(nrow(pos_set_train)/nrow(subset_feature_vector))\n",
    "    prob_sub_log<-log(nrow(neg_set_train)/nrow(subset_feature_vector))\n",
    "    \n",
    "    # gets the data that was NOT included by the createDataPartition function\n",
    "    filtered_data_feature<-feature_vector[-subset_data, ]\n",
    "    \n",
    "    # gets the classification data that was NOT included by the createDataPartition function    \n",
    "    filtered_data_class<-class_vector[-subset_data]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Now that the filtering of data is done, we can actually create the classifier\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # computes the mean with all NA values removed\n",
    "    pos_set_train_mean<-sapply(pos_set_train, mean, na.rm=TRUE)\n",
    "    neg_set_train_mean<-sapply(neg_set_train, mean, na.rm=TRUE)\n",
    "\n",
    "    # computes the standard deviation with all the NA values removed    \n",
    "    pos_set_train_sd<-sapply(pos_set_train, sd, na.rm=TRUE)\n",
    "    neg_set_train_sd<-sapply(neg_set_train, sd, na.rm=TRUE)\n",
    "\n",
    "\n",
    "    # computes the \"left term\" in naive bayes formula    \n",
    "    temp_var<-t(t(subset_feature_vector)-pos_set_train_mean)\n",
    "    left_term<-t(t(temp_var)/pos_set_train_sd)\n",
    "    # generates the probability for the\n",
    "    pos_set_train_logs<-(-(1/2)*rowSums(apply(left_term,c(1, 2), function(x)x^2), na.rm=TRUE)-sum(log(pos_set_train_sd))) + prob_add_log \n",
    "\n",
    "    # same steps as above, except for the negative training set    \n",
    "    temp_var<-t(t(subset_feature_vector)-neg_set_train_mean)\n",
    "    left_term<-t(t(temp_var)/neg_set_train_sd)\n",
    "    neg_set_train_logs<-(-(1/2)*rowSums(apply(left_term,c(1, 2), function(x)x^2), na.rm=TRUE)-sum(log(neg_set_train_sd))) + prob_sub_log\n",
    "\n",
    "    # compare the values and classify them in comparison to the values in the 9th column    \n",
    "    compared_vals<-pos_set_train_logs>neg_set_train_logs\n",
    "    results_train<-compared_vals==subset_class_vector\n",
    "    train_score[iteration]<-sum(results_train)/(sum(results_train)+sum(!results_train))\n",
    "\n",
    "\n",
    "\n",
    "    # repeat the few steps for the testing set\n",
    "    temp_var<-t(t(filtered_data_feature)-pos_set_train_mean)\n",
    "    left_term<-t(t(temp_var)/pos_set_train_sd)\n",
    "    pos_set_test_logs<-(-(1/2)*rowSums(apply(left_term,c(1, 2), function(x)x^2), na.rm=TRUE)-sum(log(pos_set_train_sd)))\n",
    "    \n",
    "    temp_var<-t(t(filtered_data_feature)-neg_set_train_mean)\n",
    "    left_term<-t(t(temp_var)/neg_set_train_sd)\n",
    "    neg_set_test_logs<-(-(1/2)*rowSums(apply(left_term,c(1, 2), function(x)x^2), na.rm=TRUE)-sum(log(neg_set_train_sd)))\n",
    "    \n",
    "    compared_vals<-pos_set_test_logs>neg_set_test_logs\n",
    "    results_test<-compared_vals==filtered_data_class\n",
    "    test_score[iteration]<-sum(results_test)/(sum(results_test)+sum(!results_test))\n",
    "}\n",
    "\n",
    "print('Training score:')                                            \n",
    "mean(train_score)\n",
    "\n",
    "print('Test score:')\n",
    "mean(test_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"PART B:\"\n",
      "[1] \"Training score:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.749918699186992"
      ],
      "text/latex": [
       "0.749918699186992"
      ],
      "text/markdown": [
       "0.749918699186992"
      ],
      "text/plain": [
       "[1] 0.7499187"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Test score:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.737908496732026"
      ],
      "text/latex": [
       "0.737908496732026"
      ],
      "text/markdown": [
       "0.737908496732026"
      ],
      "text/plain": [
       "[1] 0.7379085"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('PART B:')\n",
    "\n",
    "# reads in all the data\n",
    "all_data<-read.csv('pima-indians-diabetes.data', header=FALSE)\n",
    "\n",
    "feature_vector<-all_data[,-c(9)]\n",
    "\n",
    "class_vector<-all_data[,9]\n",
    "train_score<-array(dim=10)\n",
    "test_score<-array(dim=10)\n",
    "\n",
    "# these are the attributes that we will treat as 0\n",
    "attributes<-c(3, 4, 6, 8)\n",
    "for(i in attributes) {\n",
    "    condition<-feature_vector[, i] == 0\n",
    "    feature_vector[condition, i] = NA\n",
    "}\n",
    "\n",
    "\n",
    "for (iteration in 1:10)\n",
    "{\n",
    "    # splits the data into a partition using 80% of the data\n",
    "    subset_data<-createDataPartition(y=class_vector, p=.8, list=FALSE)\n",
    "\n",
    "    # filters subset_data to get the 80% of the rows that were present in subset_data\n",
    "    # the filter is provided by the feature_vectors variable    \n",
    "    subset_feature_vector<-feature_vector[subset_data, ]\n",
    "    \n",
    "    # this essentially gets the 9th column from the subset_data    \n",
    "    subset_class_vector<-class_vector[subset_data]\n",
    "    \n",
    "    # positive training set - I define it as the training set whose classification is equal to 1\n",
    "    # this is another filtering process\n",
    "    pos_set_train<-subset_feature_vector[subset_class_vector > 0, ]\n",
    "    \n",
    "    # negative training set - pretty much the opposite as above\n",
    "    # this is another filtering process, but does not derive from pos_set\n",
    "    neg_set_train<-subset_feature_vector[!(subset_class_vector > 0),]\n",
    "\n",
    "    # this is the log of the prior that we are going to add in (part of Naive Bayes)\n",
    "    prob_add_log<-log(nrow(pos_set_train)/nrow(subset_feature_vector))\n",
    "    prob_sub_log<-log(nrow(neg_set_train)/nrow(subset_feature_vector))\n",
    "    \n",
    "    # gets the data that was NOT included by the createDataPartition function\n",
    "    filtered_data_feature<-feature_vector[-subset_data, ]\n",
    "    \n",
    "    # gets the classification data that was NOT included by the createDataPartition function    \n",
    "    filtered_data_class<-class_vector[-subset_data]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Now that the filtering of data is done, we can actually create the classifier\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # computes the mean with all NA values removed\n",
    "    pos_set_train_mean<-sapply(pos_set_train, mean, na.rm=TRUE)\n",
    "    neg_set_train_mean<-sapply(neg_set_train, mean, na.rm=TRUE)\n",
    "\n",
    "    # computes the standard deviation with all the NA values removed    \n",
    "    pos_set_train_sd<-sapply(pos_set_train, sd, na.rm=TRUE)\n",
    "    neg_set_train_sd<-sapply(neg_set_train, sd, na.rm=TRUE)\n",
    "\n",
    "\n",
    "\n",
    "    # computes the \"left term\" in naive bayes formula    \n",
    "    temp_var<-t(t(subset_feature_vector)-pos_set_train_mean)\n",
    "    left_term<-t(t(temp_var)/pos_set_train_sd)\n",
    "    # generates the probability for the\n",
    "    pos_set_train_logs<-(-(1/2)*rowSums(apply(left_term,c(1, 2), function(x)x^2), na.rm=TRUE)-sum(log(pos_set_train_sd))) + prob_add_log\n",
    "\n",
    "    # same steps as above, except for the negative training set    \n",
    "    temp_var<-t(t(subset_feature_vector)-neg_set_train_mean)\n",
    "    left_term<-t(t(temp_var)/neg_set_train_sd)\n",
    "    neg_set_train_logs<-(-(1/2)*rowSums(apply(left_term,c(1, 2), function(x)x^2), na.rm=TRUE)-sum(log(neg_set_train_sd))) + prob_sub_log\n",
    "\n",
    "    # compare the values and classify them in comparison to the values in the 9th column    \n",
    "    compared_vals<-pos_set_train_logs>neg_set_train_logs\n",
    "    results_train<-compared_vals==subset_class_vector\n",
    "    train_score[iteration]<-sum(results_train)/(sum(results_train)+sum(!results_train))\n",
    "\n",
    "\n",
    "\n",
    "    # repeat the few steps for the testing set\n",
    "    temp_var<-t(t(filtered_data_feature)-pos_set_train_mean)\n",
    "    left_term<-t(t(temp_var)/pos_set_train_sd)\n",
    "    pos_set_test_logs<-(-(1/2)*rowSums(apply(left_term,c(1, 2), function(x)x^2), na.rm=TRUE)-sum(log(pos_set_train_sd)))\n",
    "    \n",
    "    temp_var<-t(t(filtered_data_feature)-neg_set_train_mean)\n",
    "    left_term<-t(t(temp_var)/neg_set_train_sd)\n",
    "    neg_set_test_logs<-(-(1/2)*rowSums(apply(left_term,c(1, 2), function(x)x^2), na.rm=TRUE)-sum(log(neg_set_train_sd)))\n",
    "    \n",
    "    compared_vals<-pos_set_test_logs>neg_set_test_logs\n",
    "    results_test<-compared_vals==filtered_data_class\n",
    "    test_score[iteration]<-sum(results_test)/(sum(results_test)+sum(!results_test))\n",
    "}\n",
    "\n",
    "print('Training score:')                                            \n",
    "mean(train_score)\n",
    "\n",
    "print('Test score:')\n",
    "mean(test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"PART C:\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction  0  1\n",
       "         0 84 17\n",
       "         1 16 36\n",
       "                                          \n",
       "               Accuracy : 0.7843          \n",
       "                 95% CI : (0.7106, 0.8466)\n",
       "    No Information Rate : 0.6536          \n",
       "    P-Value [Acc > NIR] : 0.0003018       \n",
       "                                          \n",
       "                  Kappa : 0.5216          \n",
       " Mcnemar's Test P-Value : 1.0000000       \n",
       "                                          \n",
       "            Sensitivity : 0.8400          \n",
       "            Specificity : 0.6792          \n",
       "         Pos Pred Value : 0.8317          \n",
       "         Neg Pred Value : 0.6923          \n",
       "             Prevalence : 0.6536          \n",
       "         Detection Rate : 0.5490          \n",
       "   Detection Prevalence : 0.6601          \n",
       "      Balanced Accuracy : 0.7596          \n",
       "                                          \n",
       "       'Positive' Class : 0               \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('PART C:')\n",
    "options(warn=-1)\n",
    "\n",
    "# reads in all the data\n",
    "all_data<-read.csv('pima-indians-diabetes.data', header=FALSE)\n",
    "\n",
    "feature_vector<-all_data[,-c(9)]\n",
    "class_vector<-as.factor(all_data[,9])\n",
    "subset_data<-createDataPartition(y=class_vector, p=.8, list=FALSE)\n",
    "\n",
    "subset_feature_vector<-feature_vector[subset_data,]\n",
    "subset_class_vector<-class_vector[subset_data]\n",
    "\n",
    "model<-train(subset_feature_vector, subset_class_vector, 'nb', trControl=trainControl(method='cv', number=10))\n",
    "test_class<-predict(model,newdata=feature_vector[-subset_data,])\n",
    "\n",
    "confusionMatrix(data=test_class, class_vector[-subset_data])\n",
    "options(warn=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"PART D:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.718954248366013"
      ],
      "text/latex": [
       "0.718954248366013"
      ],
      "text/markdown": [
       "0.718954248366013"
      ],
      "text/plain": [
       "[1] 0.7189542"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('PART D:')\n",
    "\n",
    "rm(list=ls())\n",
    "all_data<-read.csv('pima-indians-diabetes.data', header=FALSE)\n",
    "feature_vector<-all_data[,-c(9)]\n",
    "class_vector<-as.factor(all_data[,9])\n",
    "subset_data<-createDataPartition(y=class_vector, p=.8, list=FALSE)\n",
    "\n",
    "svm<-svmlight(feature_vector[subset_data,], class_vector[subset_data], pathsvm='~/Desktop/AML/hw1/')\n",
    "\n",
    "labels<-predict(svm, feature_vector[-subset_data,])\n",
    "foo<-labels$class\n",
    "sum(foo==class_vector[-subset_data])/(sum(foo==class_vector[-subset_data])+sum(!(foo==class_vector[-subset_data])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"3.3\"\n",
      "[1] \"PART A:\"\n",
      "[1] \"Mean:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.829545454545455"
      ],
      "text/latex": [
       "0.829545454545455"
      ],
      "text/markdown": [
       "0.829545454545455"
      ],
      "text/plain": [
       "[1] 0.8295455"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Standard Deviation:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.0548915785840509"
      ],
      "text/latex": [
       "0.0548915785840509"
      ],
      "text/markdown": [
       "0.0548915785840509"
      ],
      "text/plain": [
       "[1] 0.05489158"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('3.3')\n",
    "print('PART A:')\n",
    "\n",
    "# suppresses annoying warnings\n",
    "options(warn=-1)\n",
    "\n",
    "# reads in all the data\n",
    "all_data<-read.csv('processed-cleveland.data', header=FALSE)\n",
    "\n",
    "# array to store our results\n",
    "ret<-array(dim=10)\n",
    "\n",
    "# marks the rows that we want to use\n",
    "indexing_zero<-all_data[, 14] > 0\n",
    "all_data[indexing_zero, 14] = 1\n",
    "\n",
    "# rest is similar to previous questions\n",
    "feature_vector<-all_data[,-c(14)]\n",
    "class_vector<-as.factor(all_data[,14])\n",
    "\n",
    "for(i in 1:10) {\n",
    "    subset_data<-createDataPartition(y=class_vector, p=.85, list=FALSE)\n",
    "\n",
    "    subset_feature_vector<-feature_vector[subset_data,]\n",
    "    subset_class_vector<-class_vector[subset_data]\n",
    "\n",
    "    model<-train(subset_feature_vector, subset_class_vector, 'nb', trControl=trainControl(method='cv', number=10))\n",
    "    test_class<-predict(model,newdata=feature_vector[-subset_data,])\n",
    "    cm_matrix<-confusionMatrix(data=test_class, class_vector[-subset_data])\n",
    "    ret[i]<-cm_matrix$overall['Accuracy']\n",
    "}\n",
    "print(\"Mean:\")\n",
    "mean(ret)\n",
    "print(\"Standard Deviation:\")\n",
    "sd(ret)\n",
    "options(warn=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"PART B:\"\n",
      "[1] \"Mean:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.586046511627907"
      ],
      "text/latex": [
       "0.586046511627907"
      ],
      "text/markdown": [
       "0.586046511627907"
      ],
      "text/plain": [
       "[1] 0.5860465"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Standard Deviation:\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.0392220484982125"
      ],
      "text/latex": [
       "0.0392220484982125"
      ],
      "text/markdown": [
       "0.0392220484982125"
      ],
      "text/plain": [
       "[1] 0.03922205"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('PART B:')\n",
    "options(warn=-1)\n",
    "\n",
    "# reads in all the data\n",
    "all_data<-read.csv('processed-cleveland.data', header=FALSE)\n",
    "\n",
    "# array to store our results\n",
    "ret<-array(dim=10)\n",
    "\n",
    "# rest is similar to above code...\n",
    "feature_vector<-all_data[,-c(14)]\n",
    "class_vector<-as.factor(all_data[,14])\n",
    "\n",
    "for(i in 1:10) {\n",
    "    subset_data<-createDataPartition(y=class_vector, p=.85, list=FALSE)\n",
    "\n",
    "    subset_feature_vector<-feature_vector[subset_data,]\n",
    "    subset_class_vector<-class_vector[subset_data]\n",
    "\n",
    "    model<-train(subset_feature_vector, subset_class_vector, 'nb', trControl=trainControl(method='cv', number=10))\n",
    "    test_class<-predict(model,newdata=feature_vector[-subset_data,])\n",
    "\n",
    "    cm_matrix<-confusionMatrix(data=test_class, class_vector[-subset_data])\n",
    "    ret[i]<-cm_matrix$overall['Accuracy']\n",
    "}\n",
    "print(\"Mean:\")\n",
    "mean(ret)\n",
    "print(\"Standard Deviation:\")\n",
    "sd(ret)\n",
    "options(warn=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
