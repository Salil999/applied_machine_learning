{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T23:33:51.688902Z",
     "start_time": "2017-04-20T23:33:49.870862Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST for beginners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T23:41:18.884267Z",
     "start_time": "2017-04-20T23:41:16.599810Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True)\n",
    "\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "  tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Train\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep MNIST for experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T23:48:51.367316Z",
     "start_time": "2017-04-20T23:48:40.344281Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def deepnn(x):\n",
    "\t\"\"\"deepnn builds the graph for a deep net for classifying digits.\n",
    "\n",
    "\tArgs:\n",
    "\t\t  x: an input tensor with the dimensions (N_examples, 784), where 784 is the\n",
    "\t\t  number of pixels in a standard MNIST image.\n",
    "\n",
    "\tReturns:\n",
    "\t\t  A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with values\n",
    "\t\t  equal to the logits of classifying the digit into one of 10 classes (the\n",
    "\t\t  digits 0-9). keep_prob is a scalar placeholder for the probability of\n",
    "\t\t  dropout.\n",
    "\t\"\"\"\n",
    "\t# Reshape to use within a convolutional neural net.\n",
    "\t# Last dimension is for \"features\" - there is only one here, since images are\n",
    "\t# grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n",
    "\tx_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "\t# First convolutional layer - maps one grayscale image to 32 feature maps.\n",
    "\tW_conv1 = weight_variable([5, 5, 1, 32])\n",
    "\tb_conv1 = bias_variable([32])\n",
    "\th_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "\t# Pooling layer - downsamples by 2X.\n",
    "\th_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "\t# Second convolutional layer -- maps 32 feature maps to 64.\n",
    "\tW_conv2 = weight_variable([5, 5, 32, 64])\n",
    "\tb_conv2 = bias_variable([64])\n",
    "\th_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "\t# Second pooling layer.\n",
    "\th_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\t# Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n",
    "\t# is down to 7x7x64 feature maps -- maps this to 1024 features.\n",
    "\tW_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "\tb_fc1 = bias_variable([1024])\n",
    "\n",
    "\th_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "\th_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\t# Dropout - controls the complexity of the model, prevents co-adaptation of\n",
    "\t# features.\n",
    "\tkeep_prob = tf.placeholder(tf.float32)\n",
    "\th_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "\t# Map the 1024 features to 10 classes, one for each digit\n",
    "\tW_fc2 = weight_variable([1024, 10])\n",
    "\tb_fc2 = bias_variable([10])\n",
    "\n",
    "\ty_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\treturn y_conv, keep_prob\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "\t\"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "\treturn tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "\t\"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
    "\treturn tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "\t\t\t\t\t\t  strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "\t\"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "\tinitial = tf.truncated_normal(shape, stddev=0.1)\n",
    "\treturn tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "\t\"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "\tinitial = tf.constant(0.1, shape=shape)\n",
    "\treturn tf.Variable(initial)\n",
    "\n",
    "\n",
    "def main(_):\n",
    "\t# Import data\n",
    "\tmnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "\n",
    "\t# Create the model\n",
    "\tx = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "\t# Define loss and optimizer\n",
    "\ty_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "\t# Build the graph for the deep net\n",
    "\ty_conv, keep_prob = deepnn(x)\n",
    "\n",
    "\tcross_entropy = tf.reduce_mean(\n",
    "\t\ttf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\ttrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\tcorrect_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\tnum_range = 1000\n",
    "    \n",
    "\twith tf.Session() as sess:\n",
    "\t\tsess.run(tf.global_variables_initializer())\n",
    "\t\tfor i in range(num_range):\n",
    "\t\t\tbatch = mnist.train.next_batch(50)\n",
    "\t\t\tif i % 100 == 0:\n",
    "\t\t\t\ttrain_accuracy = accuracy.eval(feed_dict={\n",
    "\t\t\t\t\tx: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "\t\t\t\tprint('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "\t\t\ttrain_step.run(\n",
    "\t\t\t\tfeed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "\t\tprint('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "\t\t\tx: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\tparser.add_argument('--data_dir', type=str,\n",
    "\t\t\t\t\t\tdefault='/tmp/tensorflow/mnist/input_data',\n",
    "\t\t\t\t\t\thelp='Directory for storing input data')\n",
    "\tFLAGS, unparsed = parser.parse_known_args()\n",
    "\ttf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T23:53:13.728096Z",
     "start_time": "2017-04-20T23:52:30.159047Z"
    }
   },
   "outputs": [],
   "source": [
    "FLAGS = None\n",
    "\n",
    "def train():\n",
    "\t# Import data\n",
    "\tmnist = input_data.read_data_sets(\".\",\n",
    "\t\t\t\t\t\t\t\t\t  one_hot=True,\n",
    "\t\t\t\t\t\t\t\t\t  fake_data=FLAGS.fake_data)\n",
    "\n",
    "\tsess = tf.InteractiveSession()\n",
    "\t# Create a multilayer model.\n",
    "\n",
    "\t# Input placeholders\n",
    "\twith tf.name_scope('input'):\n",
    "\t\tx = tf.placeholder(tf.float32, [None, 784], name='x-input')\n",
    "\t\ty_ = tf.placeholder(tf.float32, [None, 10], name='y-input')\n",
    "\n",
    "\twith tf.name_scope('input_reshape'):\n",
    "\t\timage_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\t\ttf.summary.image('input', image_shaped_input, 10)\n",
    "\n",
    "\t# We can't initialize these variables to 0 - the network will get stuck.\n",
    "\tdef weight_variable(shape):\n",
    "\t\t\"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "\t\tinitial = tf.truncated_normal(shape, stddev=0.1)\n",
    "\t\treturn tf.Variable(initial)\n",
    "\n",
    "\tdef bias_variable(shape):\n",
    "\t\t\"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "\t\tinitial = tf.constant(0.1, shape=shape)\n",
    "\t\treturn tf.Variable(initial)\n",
    "\n",
    "\tdef variable_summaries(var):\n",
    "\t\t\"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "\t\twith tf.name_scope('summaries'):\n",
    "\t\t\tmean = tf.reduce_mean(var)\n",
    "\t\t\ttf.summary.scalar('mean', mean)\n",
    "\t\t\twith tf.name_scope('stddev'):\n",
    "\t\t\t\tstddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "\t\t\ttf.summary.scalar('stddev', stddev)\n",
    "\t\t\ttf.summary.scalar('max', tf.reduce_max(var))\n",
    "\t\t\ttf.summary.scalar('min', tf.reduce_min(var))\n",
    "\t\t\ttf.summary.histogram('histogram', var)\n",
    "\n",
    "\tdef nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "\t\t\"\"\"Reusable code for making a simple neural net layer.\n",
    "\n",
    "\t\tIt does a matrix multiply, bias add, and then uses ReLU to nonlinearize.\n",
    "\t\tIt also sets up name scoping so that the resultant graph is easy to read,\n",
    "\t\tand adds a number of summary ops.\n",
    "\t\t\"\"\"\n",
    "\t\t# Adding a name scope ensures logical grouping of the layers in the\n",
    "\t\t# graph.\n",
    "\t\twith tf.name_scope(layer_name):\n",
    "\t\t\t# This Variable will hold the state of the weights for the layer\n",
    "\t\t\twith tf.name_scope('weights'):\n",
    "\t\t\t\tweights = weight_variable([input_dim, output_dim])\n",
    "\t\t\t\tvariable_summaries(weights)\n",
    "\t\t\twith tf.name_scope('biases'):\n",
    "\t\t\t\tbiases = bias_variable([output_dim])\n",
    "\t\t\t\tvariable_summaries(biases)\n",
    "\t\t\twith tf.name_scope('Wx_plus_b'):\n",
    "\t\t\t\tpreactivate = tf.matmul(input_tensor, weights) + biases\n",
    "\t\t\t\ttf.summary.histogram('pre_activations', preactivate)\n",
    "\t\t\tactivations = act(preactivate, name='activation')\n",
    "\t\t\ttf.summary.histogram('activations', activations)\n",
    "\t\t\treturn activations\n",
    "\n",
    "\thidden1 = nn_layer(x, 784, 500, 'layer1')\n",
    "\n",
    "\twith tf.name_scope('dropout'):\n",
    "\t\tkeep_prob = tf.placeholder(tf.float32)\n",
    "\t\ttf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "\t\tdropped = tf.nn.dropout(hidden1, keep_prob)\n",
    "\n",
    "\t# Do not apply softmax activation yet, see below.\n",
    "\ty = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity)\n",
    "\n",
    "\twith tf.name_scope('cross_entropy'):\n",
    "\t\t# The raw formulation of cross-entropy,\n",
    "\t\t#\n",
    "\t\t# tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n",
    "\t\t#                               reduction_indices=[1]))\n",
    "\t\t#\n",
    "\t\t# can be numerically unstable.\n",
    "\t\t#\n",
    "\t\t# So here we use tf.nn.softmax_cross_entropy_with_logits on the\n",
    "\t\t# raw outputs of the nn_layer above, and then average across\n",
    "\t\t# the batch.\n",
    "\t\tdiff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "\t\twith tf.name_scope('total'):\n",
    "\t\t\tcross_entropy = tf.reduce_mean(diff)\n",
    "\ttf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "\twith tf.name_scope('train'):\n",
    "\t\ttrain_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n",
    "\t\t\tcross_entropy)\n",
    "\n",
    "\twith tf.name_scope('accuracy'):\n",
    "\t\twith tf.name_scope('correct_prediction'):\n",
    "\t\t\tcorrect_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "\t\twith tf.name_scope('accuracy'):\n",
    "\t\t\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\ttf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "\t# Merge all the summaries and write them out to\n",
    "\t# /tmp/tensorflow/mnist/logs/mnist_with_summaries (by default)\n",
    "\tmerged = tf.summary.merge_all()\n",
    "\ttrain_writer = tf.summary.FileWriter(FLAGS.log_dir + '/train', sess.graph)\n",
    "\ttest_writer = tf.summary.FileWriter(FLAGS.log_dir + '/test')\n",
    "\ttf.global_variables_initializer().run()\n",
    "\n",
    "\t# Train the model, and also write summaries.\n",
    "\t# Every 10th step, measure test-set accuracy, and write test summaries\n",
    "\t# All other steps, run train_step on training data, & add training\n",
    "\t# summaries\n",
    "\n",
    "\tdef feed_dict(train):\n",
    "\t\t\"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "\t\tif train or FLAGS.fake_data:\n",
    "\t\t\txs, ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)\n",
    "\t\t\tk = FLAGS.dropout\n",
    "\t\telse:\n",
    "\t\t\txs, ys = mnist.test.images, mnist.test.labels\n",
    "\t\t\tk = 1.0\n",
    "\t\treturn {x: xs, y_: ys, keep_prob: k}\n",
    "\n",
    "\tfor i in range(2500):\n",
    "\t\tif i % 100 == 0:  # Record summaries and test-set accuracy\n",
    "\t\t\tsummary, acc = sess.run(\n",
    "\t\t\t\t[merged, accuracy], feed_dict=feed_dict(False))\n",
    "\t\t\ttest_writer.add_summary(summary, i)\n",
    "\t\t\tprint('Accuracy at step %s: %s' % (i, acc))\n",
    "\t\telse:  # Record train set summaries, and train\n",
    "\t\t\tif i % 100 == 0:  # Record execution stats\n",
    "\t\t\t\trun_options = tf.RunOptions(\n",
    "\t\t\t\t\ttrace_level=tf.RunOptions.FULL_TRACE)\n",
    "\t\t\t\trun_metadata = tf.RunMetadata()\n",
    "\t\t\t\tsummary, _ = sess.run([merged, train_step],\n",
    "\t\t\t\t\t\t\t\t\t  feed_dict=feed_dict(True),\n",
    "\t\t\t\t\t\t\t\t\t  options=run_options,\n",
    "\t\t\t\t\t\t\t\t\t  run_metadata=run_metadata)\n",
    "\t\t\t\ttrain_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
    "\t\t\t\ttrain_writer.add_summary(summary, i)\n",
    "\t\t\t\tprint('Adding run metadata for', i)\n",
    "\t\t\telse:  # Record a summary\n",
    "\t\t\t\tsummary, _ = sess.run(\n",
    "\t\t\t\t\t[merged, train_step], feed_dict=feed_dict(True))\n",
    "\t\t\t\ttrain_writer.add_summary(summary, i)\n",
    "\ttrain_writer.close()\n",
    "\ttest_writer.close()\n",
    "\n",
    "\n",
    "def main(_):\n",
    "\tif tf.gfile.Exists(FLAGS.log_dir):\n",
    "\t\ttf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
    "\ttf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "\ttrain()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\tparser.add_argument('--fake_data', nargs='?', const=True, type=bool,\n",
    "\t\t\t\t\t\tdefault=False,\n",
    "\t\t\t\t\t\thelp='If true, uses fake data for unit testing.')\n",
    "\tparser.add_argument('--max_steps', type=int, default=1000,\n",
    "\t\t\t\t\t\thelp='Number of steps to run trainer.')\n",
    "\tparser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "\t\t\t\t\t\thelp='Initial learning rate')\n",
    "\tparser.add_argument('--dropout', type=float, default=0.9,\n",
    "\t\t\t\t\t\thelp='Keep probability for training dropout.')\n",
    "\tparser.add_argument('--data_dir', type=str,\n",
    "\t\t\t\t\t\tdefault='/tmp/tensorflow/mnist/input_data',\n",
    "\t\t\t\t\t\thelp='Directory for storing input data')\n",
    "\tparser.add_argument('--log_dir', type=str,\n",
    "\t\t\t\t\t\tdefault='/tmp/tensorflow/mnist/logs/mnist_with_summaries',\n",
    "\t\t\t\t\t\thelp='Summaries log directory')\n",
    "\tFLAGS, unparsed = parser.parse_known_args()\n",
    "\ttf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T23:33:58.293626Z",
     "start_time": "2017-04-20T23:33:57.043Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Routine for decoding the CIFAR-10 binary file format.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "# Process images of this size. Note that this differs from the original CIFAR\n",
    "# image size of 32 x 32. If one alters this number, then the entire model\n",
    "# architecture will change and any model would need to be retrained.\n",
    "IMAGE_SIZE = 24\n",
    "\n",
    "# Global constants describing the CIFAR-10 data set.\n",
    "NUM_CLASSES = 10\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n",
    "\n",
    "\n",
    "def read_cifar10(filename_queue):\n",
    "\t\"\"\"Reads and parses examples from CIFAR10 data files.\n",
    "\n",
    "\tRecommendation: if you want N-way read parallelism, call this function\n",
    "\tN times.  This will give you N independent Readers reading different\n",
    "\tfiles & positions within those files, which will give better mixing of\n",
    "\texamples.\n",
    "\n",
    "\tArgs:\n",
    "\t\t  filename_queue: A queue of strings with the filenames to read from.\n",
    "\n",
    "\tReturns:\n",
    "\t\t  An object representing a single example, with the following fields:\n",
    "\t\t\theight: number of rows in the result (32)\n",
    "\t\t\twidth: number of columns in the result (32)\n",
    "\t\t\tdepth: number of color channels in the result (3)\n",
    "\t\t\tkey: a scalar string Tensor describing the filename & record number\n",
    "\t\t\t\t  for this example.\n",
    "\t\t\tlabel: an int32 Tensor with the label in the range 0..9.\n",
    "\t\t\tuint8image: a [height, width, depth] uint8 Tensor with the image data\n",
    "\t\"\"\"\n",
    "\n",
    "\tclass CIFAR10Record(object):\n",
    "\t\tpass\n",
    "\tresult = CIFAR10Record()\n",
    "\n",
    "\t# Dimensions of the images in the CIFAR-10 dataset.\n",
    "\t# See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the\n",
    "\t# input format.\n",
    "\tlabel_bytes = 1  # 2 for CIFAR-100\n",
    "\tresult.height = 32\n",
    "\tresult.width = 32\n",
    "\tresult.depth = 3\n",
    "\timage_bytes = result.height * result.width * result.depth\n",
    "\t# Every record consists of a label followed by the image, with a\n",
    "\t# fixed number of bytes for each.\n",
    "\trecord_bytes = label_bytes + image_bytes\n",
    "\n",
    "\t# Read a record, getting filenames from the filename_queue.  No\n",
    "\t# header or footer in the CIFAR-10 format, so we leave header_bytes\n",
    "\t# and footer_bytes at their default of 0.\n",
    "\treader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n",
    "\tresult.key, value = reader.read(filename_queue)\n",
    "\n",
    "\t# Convert from a string to a vector of uint8 that is record_bytes long.\n",
    "\trecord_bytes = tf.decode_raw(value, tf.uint8)\n",
    "\n",
    "\t# The first bytes represent the label, which we convert from uint8->int32.\n",
    "\tresult.label = tf.cast(\n",
    "\t\ttf.strided_slice(record_bytes, [0], [label_bytes]), tf.int32)\n",
    "\n",
    "\t# The remaining bytes after the label represent the image, which we reshape\n",
    "\t# from [depth * height * width] to [depth, height, width].\n",
    "\tdepth_major = tf.reshape(\n",
    "\t\ttf.strided_slice(record_bytes, [label_bytes],\n",
    "\t\t\t\t\t\t [label_bytes + image_bytes]),\n",
    "\t\t[result.depth, result.height, result.width])\n",
    "\t# Convert from [depth, height, width] to [height, width, depth].\n",
    "\tresult.uint8image = tf.transpose(depth_major, [1, 2, 0])\n",
    "\n",
    "\treturn result\n",
    "\n",
    "\n",
    "def _generate_image_and_label_batch(image, label, min_queue_examples,\n",
    "\t\t\t\t\t\t\t\t\tbatch_size, shuffle):\n",
    "\t\"\"\"Construct a queued batch of images and labels.\n",
    "\n",
    "\tArgs:\n",
    "\t\t  image: 3-D Tensor of [height, width, 3] of type.float32.\n",
    "\t\t  label: 1-D Tensor of type.int32\n",
    "\t\t  min_queue_examples: int32, minimum number of samples to retain\n",
    "\t\t\tin the queue that provides of batches of examples.\n",
    "\t\t  batch_size: Number of images per batch.\n",
    "\t\t  shuffle: boolean indicating whether to use a shuffling queue.\n",
    "\n",
    "\tReturns:\n",
    "\t\t  images: Images. 4D tensor of [batch_size, height, width, 3] size.\n",
    "\t\t  labels: Labels. 1D tensor of [batch_size] size.\n",
    "\t\"\"\"\n",
    "\t# Create a queue that shuffles the examples, and then\n",
    "\t# read 'batch_size' images + labels from the example queue.\n",
    "\tnum_preprocess_threads = 16\n",
    "\tif shuffle:\n",
    "\t\timages, label_batch = tf.train.shuffle_batch(\n",
    "\t\t\t[image, label],\n",
    "\t\t\tbatch_size=batch_size,\n",
    "\t\t\tnum_threads=num_preprocess_threads,\n",
    "\t\t\tcapacity=min_queue_examples + 3 * batch_size,\n",
    "\t\t\tmin_after_dequeue=min_queue_examples)\n",
    "\telse:\n",
    "\t\timages, label_batch = tf.train.batch(\n",
    "\t\t\t[image, label],\n",
    "\t\t\tbatch_size=batch_size,\n",
    "\t\t\tnum_threads=num_preprocess_threads,\n",
    "\t\t\tcapacity=min_queue_examples + 3 * batch_size)\n",
    "\n",
    "\t# Display the training images in the visualizer.\n",
    "\ttf.summary.image('images', images)\n",
    "\n",
    "\treturn images, tf.reshape(label_batch, [batch_size])\n",
    "\n",
    "\n",
    "def distorted_inputs(data_dir, batch_size):\n",
    "\t\"\"\"Construct distorted input for CIFAR training using the Reader ops.\n",
    "\n",
    "\tArgs:\n",
    "\t\t  data_dir: Path to the CIFAR-10 data directory.\n",
    "\t\t  batch_size: Number of images per batch.\n",
    "\n",
    "\tReturns:\n",
    "\t\t  images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n",
    "\t\t  labels: Labels. 1D tensor of [batch_size] size.\n",
    "\t\"\"\"\n",
    "\tfilenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i)\n",
    "\t\t\t\t for i in xrange(1, 6)]\n",
    "\tfor f in filenames:\n",
    "\t\tif not tf.gfile.Exists(f):\n",
    "\t\t\traise ValueError('Failed to find file: ' + f)\n",
    "\n",
    "\t# Create a queue that produces the filenames to read.\n",
    "\tfilename_queue = tf.train.string_input_producer(filenames)\n",
    "\n",
    "\t# Read examples from files in the filename queue.\n",
    "\tread_input = read_cifar10(filename_queue)\n",
    "\treshaped_image = tf.cast(read_input.uint8image, tf.float32)\n",
    "\n",
    "\theight = IMAGE_SIZE\n",
    "\twidth = IMAGE_SIZE\n",
    "\n",
    "\t# Image processing for training the network. Note the many random\n",
    "\t# distortions applied to the image.\n",
    "\n",
    "\t# Randomly crop a [height, width] section of the image.\n",
    "\tdistorted_image = tf.random_crop(reshaped_image, [height, width, 3])\n",
    "\n",
    "\t# Randomly flip the image horizontally.\n",
    "\tdistorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "\n",
    "\t# Because these operations are not commutative, consider randomizing\n",
    "\t# the order their operation.\n",
    "\tdistorted_image = tf.image.random_brightness(distorted_image,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t max_delta=63)\n",
    "\tdistorted_image = tf.image.random_contrast(distorted_image,\n",
    "\t\t\t\t\t\t\t\t\t\t\t   lower=0.2, upper=1.8)\n",
    "\n",
    "\t# Subtract off the mean and divide by the variance of the pixels.\n",
    "\tfloat_image = tf.image.per_image_standardization(distorted_image)\n",
    "\n",
    "\t# Set the shapes of tensors.\n",
    "\tfloat_image.set_shape([height, width, 3])\n",
    "\tread_input.label.set_shape([1])\n",
    "\n",
    "\t# Ensure that the random shuffling has good mixing properties.\n",
    "\tmin_fraction_of_examples_in_queue = 0.4\n",
    "\tmin_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN *\n",
    "\t\t\t\t\t\t\t min_fraction_of_examples_in_queue)\n",
    "\tprint('Filling queue with %d CIFAR images before starting to train. '\n",
    "\t\t  'This will take a few minutes.' % min_queue_examples)\n",
    "\n",
    "\t# Generate a batch of images and labels by building up a queue of examples.\n",
    "\treturn _generate_image_and_label_batch(float_image, read_input.label,\n",
    "\t\t\t\t\t\t\t\t\t\t   min_queue_examples, batch_size,\n",
    "\t\t\t\t\t\t\t\t\t\t   shuffle=True)\n",
    "\n",
    "\n",
    "def inputs(eval_data, data_dir, batch_size):\n",
    "\t\"\"\"Construct input for CIFAR evaluation using the Reader ops.\n",
    "\n",
    "\tArgs:\n",
    "\t\t  eval_data: bool, indicating if one should use the train or eval data set.\n",
    "\t\t  data_dir: Path to the CIFAR-10 data directory.\n",
    "\t\t  batch_size: Number of images per batch.\n",
    "\n",
    "\tReturns:\n",
    "\t\t  images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n",
    "\t\t  labels: Labels. 1D tensor of [batch_size] size.\n",
    "\t\"\"\"\n",
    "\tif not eval_data:\n",
    "\t\tfilenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i)\n",
    "\t\t\t\t\t for i in xrange(1, 6)]\n",
    "\t\tnum_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "\telse:\n",
    "\t\tfilenames = [os.path.join(data_dir, 'test_batch.bin')]\n",
    "\t\tnum_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\n",
    "\n",
    "\tfor f in filenames:\n",
    "\t\tif not tf.gfile.Exists(f):\n",
    "\t\t\traise ValueError('Failed to find file: ' + f)\n",
    "\n",
    "\t# Create a queue that produces the filenames to read.\n",
    "\tfilename_queue = tf.train.string_input_producer(filenames)\n",
    "\n",
    "\t# Read examples from files in the filename queue.\n",
    "\tread_input = read_cifar10(filename_queue)\n",
    "\treshaped_image = tf.cast(read_input.uint8image, tf.float32)\n",
    "\n",
    "\theight = IMAGE_SIZE\n",
    "\twidth = IMAGE_SIZE\n",
    "\n",
    "\t# Image processing for evaluation.\n",
    "\t# Crop the central [height, width] of the image.\n",
    "\tresized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t   height, width)\n",
    "\n",
    "\t# Subtract off the mean and divide by the variance of the pixels.\n",
    "\tfloat_image = tf.image.per_image_standardization(resized_image)\n",
    "\n",
    "\t# Set the shapes of tensors.\n",
    "\tfloat_image.set_shape([height, width, 3])\n",
    "\tread_input.label.set_shape([1])\n",
    "\n",
    "\t# Ensure that the random shuffling has good mixing properties.\n",
    "\tmin_fraction_of_examples_in_queue = 0.4\n",
    "\tmin_queue_examples = int(num_examples_per_epoch *\n",
    "\t\t\t\t\t\t\t min_fraction_of_examples_in_queue)\n",
    "\n",
    "\t# Generate a batch of images and labels by building up a queue of examples.\n",
    "\treturn _generate_image_and_label_batch(float_image, read_input.label,\n",
    "\t\t\t\t\t\t\t\t\t\t   min_queue_examples, batch_size,\n",
    "\t\t\t\t\t\t\t\t\t\t   shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T23:33:58.295442Z",
     "start_time": "2017-04-20T23:33:57.882Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Builds the CIFAR-10 network.\n",
    "\n",
    "Summary of available functions:\n",
    "\n",
    " # Compute input images and labels for training. If you would like to run\n",
    " # evaluations, use inputs() instead.\n",
    " inputs, labels = distorted_inputs()\n",
    "\n",
    " # Compute inference on the model inputs to make a prediction.\n",
    " predictions = inference(inputs)\n",
    "\n",
    " # Compute the total loss of the prediction with respect to the labels.\n",
    " loss = loss(predictions, labels)\n",
    "\n",
    " # Create a graph to run one step of training with respect to the loss.\n",
    " train_op = train(loss, global_step)\n",
    "\"\"\"\n",
    "# pylint: disable=missing-docstring\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "\n",
    "import cifar10_input\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# Basic model parameters.\n",
    "tf.app.flags.DEFINE_integer('batch_size', 128,\n",
    "\t\t\t\t\t\t\t\"\"\"Number of images to process in a batch.\"\"\")\n",
    "tf.app.flags.DEFINE_string('data_dir', '/tmp/cifar10_data',\n",
    "\t\t\t\t\t\t   \"\"\"Path to the CIFAR-10 data directory.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('use_fp16', False,\n",
    "\t\t\t\t\t\t\t\"\"\"Train the model using fp16.\"\"\")\n",
    "\n",
    "# Global constants describing the CIFAR-10 data set.\n",
    "IMAGE_SIZE = cifar10_input.IMAGE_SIZE\n",
    "NUM_CLASSES = cifar10_input.NUM_CLASSES\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\n",
    "\n",
    "\n",
    "# Constants describing the training process.\n",
    "MOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\n",
    "NUM_EPOCHS_PER_DECAY = 350.0      # Epochs after which learning rate decays.\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\n",
    "INITIAL_LEARNING_RATE = 0.1       # Initial learning rate.\n",
    "\n",
    "# If a model is trained with multiple GPUs, prefix all Op names with tower_name\n",
    "# to differentiate the operations. Note that this prefix is removed from the\n",
    "# names of the summaries when visualizing a model.\n",
    "TOWER_NAME = 'tower'\n",
    "\n",
    "DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "\n",
    "\n",
    "def _activation_summary(x):\n",
    "\t\"\"\"Helper to create summaries for activations.\n",
    "\n",
    "\tCreates a summary that provides a histogram of activations.\n",
    "\tCreates a summary that measures the sparsity of activations.\n",
    "\n",
    "\tArgs:\n",
    "\t  x: Tensor\n",
    "\tReturns:\n",
    "\t  nothing\n",
    "\t\"\"\"\n",
    "\t# Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "\t# session. This helps the clarity of presentation on tensorboard.\n",
    "\ttensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n",
    "\ttf.summary.histogram(tensor_name + '/activations', x)\n",
    "\ttf.summary.scalar(tensor_name + '/sparsity',\n",
    "\t\t\t\t\t  tf.nn.zero_fraction(x))\n",
    "\n",
    "\n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "\t\"\"\"Helper to create a Variable stored on CPU memory.\n",
    "\n",
    "\tArgs:\n",
    "\t  name: name of the variable\n",
    "\t  shape: list of ints\n",
    "\t  initializer: initializer for Variable\n",
    "\n",
    "\tReturns:\n",
    "\t  Variable Tensor\n",
    "\t\"\"\"\n",
    "\twith tf.device('/cpu:0'):\n",
    "\t\tdtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n",
    "\t\tvar = tf.get_variable(\n",
    "\t\t\tname, shape, initializer=initializer, dtype=dtype)\n",
    "\treturn var\n",
    "\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "\t\"\"\"Helper to create an initialized Variable with weight decay.\n",
    "\n",
    "\tNote that the Variable is initialized with a truncated normal distribution.\n",
    "\tA weight decay is added only if one is specified.\n",
    "\n",
    "\tArgs:\n",
    "\t  name: name of the variable\n",
    "\t  shape: list of ints\n",
    "\t  stddev: standard deviation of a truncated Gaussian\n",
    "\t  wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "\t\t  decay is not added for this Variable.\n",
    "\n",
    "\tReturns:\n",
    "\t  Variable Tensor\n",
    "\t\"\"\"\n",
    "\tdtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n",
    "\tvar = _variable_on_cpu(\n",
    "\t\tname,\n",
    "\t\tshape,\n",
    "\t\ttf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "\tif wd is not None:\n",
    "\t\tweight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "\t\ttf.add_to_collection('losses', weight_decay)\n",
    "\treturn var\n",
    "\n",
    "\n",
    "def distorted_inputs():\n",
    "\t\"\"\"Construct distorted input for CIFAR training using the Reader ops.\n",
    "\n",
    "\tReturns:\n",
    "\t  images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n",
    "\t  labels: Labels. 1D tensor of [batch_size] size.\n",
    "\n",
    "\tRaises:\n",
    "\t  ValueError: If no data_dir\n",
    "\t\"\"\"\n",
    "\tif not FLAGS.data_dir:\n",
    "\t\traise ValueError('Please supply a data_dir')\n",
    "\tdata_dir = os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin')\n",
    "\timages, labels = cifar10_input.distorted_inputs(data_dir=data_dir,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tbatch_size=FLAGS.batch_size)\n",
    "\tif FLAGS.use_fp16:\n",
    "\t\timages = tf.cast(images, tf.float16)\n",
    "\t\tlabels = tf.cast(labels, tf.float16)\n",
    "\treturn images, labels\n",
    "\n",
    "\n",
    "def inputs(eval_data):\n",
    "\t\"\"\"Construct input for CIFAR evaluation using the Reader ops.\n",
    "\n",
    "\tArgs:\n",
    "\t  eval_data: bool, indicating if one should use the train or eval data set.\n",
    "\n",
    "\tReturns:\n",
    "\t  images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n",
    "\t  labels: Labels. 1D tensor of [batch_size] size.\n",
    "\n",
    "\tRaises:\n",
    "\t  ValueError: If no data_dir\n",
    "\t\"\"\"\n",
    "\tif not FLAGS.data_dir:\n",
    "\t\traise ValueError('Please supply a data_dir')\n",
    "\tdata_dir = os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin')\n",
    "\timages, labels = cifar10_input.inputs(eval_data=eval_data,\n",
    "\t\t\t\t\t\t\t\t\t\t  data_dir=data_dir,\n",
    "\t\t\t\t\t\t\t\t\t\t  batch_size=FLAGS.batch_size)\n",
    "\tif FLAGS.use_fp16:\n",
    "\t\timages = tf.cast(images, tf.float16)\n",
    "\t\tlabels = tf.cast(labels, tf.float16)\n",
    "\treturn images, labels\n",
    "\n",
    "\n",
    "def inference(images):\n",
    "\t\"\"\"Build the CIFAR-10 model.\n",
    "\n",
    "\tArgs:\n",
    "\t  images: Images returned from distorted_inputs() or inputs().\n",
    "\n",
    "\tReturns:\n",
    "\t  Logits.\n",
    "\t\"\"\"\n",
    "\t# We instantiate all variables using tf.get_variable() instead of\n",
    "\t# tf.Variable() in order to share variables across multiple GPU training runs.\n",
    "\t# If we only ran this model on a single GPU, we could simplify this function\n",
    "\t# by replacing all instances of tf.get_variable() with tf.Variable().\n",
    "\t#\n",
    "\t# conv1\n",
    "\twith tf.variable_scope('conv1') as scope:\n",
    "\t\tkernel = _variable_with_weight_decay('weights',\n",
    "\t\t\t\t\t\t\t\t\t\t\t shape=[5, 5, 3, 64],\n",
    "\t\t\t\t\t\t\t\t\t\t\t stddev=5e-2,\n",
    "\t\t\t\t\t\t\t\t\t\t\t wd=0.0)\n",
    "\t\tconv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "\t\tbiases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "\t\tpre_activation = tf.nn.bias_add(conv, biases)\n",
    "\t\tconv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\t\t_activation_summary(conv1)\n",
    "\n",
    "\t# pool1\n",
    "\tpool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "\t\t\t\t\t\t   padding='SAME', name='pool1')\n",
    "\t# norm1\n",
    "\tnorm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "\t\t\t\t\t  name='norm1')\n",
    "\n",
    "\t# conv2\n",
    "\twith tf.variable_scope('conv2') as scope:\n",
    "\t\tkernel = _variable_with_weight_decay('weights',\n",
    "\t\t\t\t\t\t\t\t\t\t\t shape=[5, 5, 64, 64],\n",
    "\t\t\t\t\t\t\t\t\t\t\t stddev=5e-2,\n",
    "\t\t\t\t\t\t\t\t\t\t\t wd=0.0)\n",
    "\t\tconv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "\t\tbiases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "\t\tpre_activation = tf.nn.bias_add(conv, biases)\n",
    "\t\tconv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\t\t_activation_summary(conv2)\n",
    "\n",
    "\t# norm2\n",
    "\tnorm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "\t\t\t\t\t  name='norm2')\n",
    "\t# pool2\n",
    "\tpool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n",
    "\t\t\t\t\t\t   strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "\n",
    "\t# local3\n",
    "\twith tf.variable_scope('local3') as scope:\n",
    "\t\t# Move everything into depth so we can perform a single matrix\n",
    "\t\t# multiply.\n",
    "\t\treshape = tf.reshape(pool2, [FLAGS.batch_size, -1])\n",
    "\t\tdim = reshape.get_shape()[1].value\n",
    "\t\tweights = _variable_with_weight_decay('weights', shape=[dim, 384],\n",
    "\t\t\t\t\t\t\t\t\t\t\t  stddev=0.04, wd=0.004)\n",
    "\t\tbiases = _variable_on_cpu(\n",
    "\t\t\t'biases', [384], tf.constant_initializer(0.1))\n",
    "\t\tlocal3 = tf.nn.relu(tf.matmul(reshape, weights) +\n",
    "\t\t\t\t\t\t\tbiases, name=scope.name)\n",
    "\t\t_activation_summary(local3)\n",
    "\n",
    "\t# local4\n",
    "\twith tf.variable_scope('local4') as scope:\n",
    "\t\tweights = _variable_with_weight_decay('weights', shape=[384, 192],\n",
    "\t\t\t\t\t\t\t\t\t\t\t  stddev=0.04, wd=0.004)\n",
    "\t\tbiases = _variable_on_cpu(\n",
    "\t\t\t'biases', [192], tf.constant_initializer(0.1))\n",
    "\t\tlocal4 = tf.nn.relu(tf.matmul(local3, weights) +\n",
    "\t\t\t\t\t\t\tbiases, name=scope.name)\n",
    "\t\t_activation_summary(local4)\n",
    "\n",
    "\t# linear layer(WX + b),\n",
    "\t# We don't apply softmax here because\n",
    "\t# tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n",
    "\t# and performs the softmax internally for efficiency.\n",
    "\twith tf.variable_scope('softmax_linear') as scope:\n",
    "\t\tweights = _variable_with_weight_decay('weights', [192, NUM_CLASSES],\n",
    "\t\t\t\t\t\t\t\t\t\t\t  stddev=1 / 192.0, wd=0.0)\n",
    "\t\tbiases = _variable_on_cpu('biases', [NUM_CLASSES],\n",
    "\t\t\t\t\t\t\t\t  tf.constant_initializer(0.0))\n",
    "\t\tsoftmax_linear = tf.add(\n",
    "\t\t\ttf.matmul(local4, weights), biases, name=scope.name)\n",
    "\t\t_activation_summary(softmax_linear)\n",
    "\n",
    "\treturn softmax_linear\n",
    "\n",
    "\n",
    "def loss(logits, labels):\n",
    "\t\"\"\"Add L2Loss to all the trainable variables.\n",
    "\n",
    "\tAdd summary for \"Loss\" and \"Loss/avg\".\n",
    "\tArgs:\n",
    "\t  logits: Logits from inference().\n",
    "\t  labels: Labels from distorted_inputs or inputs(). 1-D tensor\n",
    "\t\t\t  of shape [batch_size]\n",
    "\n",
    "\tReturns:\n",
    "\t  Loss tensor of type float.\n",
    "\t\"\"\"\n",
    "\t# Calculate the average cross entropy loss across the batch.\n",
    "\tlabels = tf.cast(labels, tf.int64)\n",
    "\tcross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "\t\tlabels=labels, logits=logits, name='cross_entropy_per_example')\n",
    "\tcross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "\ttf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "\t# The total loss is defined as the cross entropy loss plus all of the weight\n",
    "\t# decay terms (L2 loss).\n",
    "\treturn tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "\n",
    "def _add_loss_summaries(total_loss):\n",
    "\t\"\"\"Add summaries for losses in CIFAR-10 model.\n",
    "\n",
    "\tGenerates moving average for all losses and associated summaries for\n",
    "\tvisualizing the performance of the network.\n",
    "\n",
    "\tArgs:\n",
    "\t  total_loss: Total loss from loss().\n",
    "\tReturns:\n",
    "\t  loss_averages_op: op for generating moving averages of losses.\n",
    "\t\"\"\"\n",
    "\t# Compute the moving average of all individual losses and the total loss.\n",
    "\tloss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "\tlosses = tf.get_collection('losses')\n",
    "\tloss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "\t# Attach a scalar summary to all individual losses and the total loss; do the\n",
    "\t# same for the averaged version of the losses.\n",
    "\tfor l in losses + [total_loss]:\n",
    "\t\t# Name each loss as '(raw)' and name the moving average version of the loss\n",
    "\t\t# as the original loss name.\n",
    "\t\ttf.summary.scalar(l.op.name + ' (raw)', l)\n",
    "\t\ttf.summary.scalar(l.op.name, loss_averages.average(l))\n",
    "\n",
    "\treturn loss_averages_op\n",
    "\n",
    "\n",
    "def train(total_loss, global_step):\n",
    "\t\"\"\"Train CIFAR-10 model.\n",
    "\n",
    "\tCreate an optimizer and apply to all trainable variables. Add moving\n",
    "\taverage for all trainable variables.\n",
    "\n",
    "\tArgs:\n",
    "\t  total_loss: Total loss from loss().\n",
    "\t  global_step: Integer Variable counting the number of training steps\n",
    "\t\tprocessed.\n",
    "\tReturns:\n",
    "\t  train_op: op for training.\n",
    "\t\"\"\"\n",
    "\t# Variables that affect learning rate.\n",
    "\tnum_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size\n",
    "\tdecay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n",
    "\n",
    "\t# Decay the learning rate exponentially based on the number of steps.\n",
    "\tlr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "\t\t\t\t\t\t\t\t\tglobal_step,\n",
    "\t\t\t\t\t\t\t\t\tdecay_steps,\n",
    "\t\t\t\t\t\t\t\t\tLEARNING_RATE_DECAY_FACTOR,\n",
    "\t\t\t\t\t\t\t\t\tstaircase=True)\n",
    "\ttf.summary.scalar('learning_rate', lr)\n",
    "\n",
    "\t# Generate moving averages of all losses and associated summaries.\n",
    "\tloss_averages_op = _add_loss_summaries(total_loss)\n",
    "\n",
    "\t# Compute gradients.\n",
    "\twith tf.control_dependencies([loss_averages_op]):\n",
    "\t\topt = tf.train.GradientDescentOptimizer(lr)\n",
    "\t\tgrads = opt.compute_gradients(total_loss)\n",
    "\n",
    "\t# Apply gradients.\n",
    "\tapply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "\t# Add histograms for trainable variables.\n",
    "\tfor var in tf.trainable_variables():\n",
    "\t\ttf.summary.histogram(var.op.name, var)\n",
    "\n",
    "\t# Add histograms for gradients.\n",
    "\tfor grad, var in grads:\n",
    "\t\tif grad is not None:\n",
    "\t\t\ttf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "\n",
    "\t# Track the moving averages of all trainable variables.\n",
    "\tvariable_averages = tf.train.ExponentialMovingAverage(\n",
    "\t\tMOVING_AVERAGE_DECAY, global_step)\n",
    "\tvariables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "\twith tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "\t\ttrain_op = tf.no_op(name='train')\n",
    "\n",
    "\treturn train_op\n",
    "\n",
    "\n",
    "def maybe_download_and_extract():\n",
    "\t\"\"\"Download and extract the tarball from Alex's website.\"\"\"\n",
    "\tdest_directory = FLAGS.data_dir\n",
    "\tif not os.path.exists(dest_directory):\n",
    "\t\tos.makedirs(dest_directory)\n",
    "\tfilename = DATA_URL.split('/')[-1]\n",
    "\tfilepath = os.path.join(dest_directory, filename)\n",
    "\tif not os.path.exists(filepath):\n",
    "\t\tdef _progress(count, block_size, total_size):\n",
    "\t\t\tsys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t float(count * block_size) / float(total_size) * 100.0))\n",
    "\t\t\tsys.stdout.flush()\n",
    "\t\tfilepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
    "\t\tprint()\n",
    "\t\tstatinfo = os.stat(filepath)\n",
    "\t\tprint('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "\textracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')\n",
    "\tif not os.path.exists(extracted_dir_path):\n",
    "\t\ttarfile.open(filepath, 'r:gz').extractall(dest_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T23:33:58.883667Z",
     "start_time": "2017-04-20T23:33:58.630338Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"A binary to train CIFAR-10 using a single GPU.\n",
    "\n",
    "Accuracy:\n",
    "cifar10_train.py achieves ~86% accuracy after 100K steps (256 epochs of\n",
    "data) as judged by cifar10_eval.py.\n",
    "\n",
    "Speed: With batch_size 128.\n",
    "\n",
    "System        | Step Time (sec/batch)  |     Accuracy\n",
    "------------------------------------------------------------------\n",
    "1 Tesla K20m  | 0.35-0.60              | ~86% at 60K steps  (5 hours)\n",
    "1 Tesla K40m  | 0.25-0.35              | ~86% at 100K steps (4 hours)\n",
    "\n",
    "Usage:\n",
    "Please see the tutorial and website for how to download the CIFAR-10\n",
    "data set, compile the program and train the model.\n",
    "\n",
    "http://tensorflow.org/tutorials/deep_cnn/\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import cifar10\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train',\n",
    "\t\t\t\t\t\t   \"\"\"Directory where to write event logs \"\"\"\n",
    "\t\t\t\t\t\t   \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps', 1000000,\n",
    "\t\t\t\t\t\t\t\"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "\t\t\t\t\t\t\t\"\"\"Whether to log device placement.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('log_frequency', 10,\n",
    "\t\t\t\t\t\t\t\"\"\"How often to log results to the console.\"\"\")\n",
    "\n",
    "\n",
    "def train():\n",
    "\t\"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
    "\twith tf.Graph().as_default():\n",
    "\t\tglobal_step = tf.contrib.framework.get_or_create_global_step()\n",
    "\n",
    "\t\t# Get images and labels for CIFAR-10.\n",
    "\t\timages, labels = cifar10.distorted_inputs()\n",
    "\n",
    "\t\t# Build a Graph that computes the logits predictions from the\n",
    "\t\t# inference model.\n",
    "\t\tlogits = cifar10.inference(images)\n",
    "\n",
    "\t\t# Calculate loss.\n",
    "\t\tloss = cifar10.loss(logits, labels)\n",
    "\n",
    "\t\t# Build a Graph that trains the model with one batch of examples and\n",
    "\t\t# updates the model parameters.\n",
    "\t\ttrain_op = cifar10.train(loss, global_step)\n",
    "\n",
    "\t\tclass _LoggerHook(tf.train.SessionRunHook):\n",
    "\t\t\t\"\"\"Logs loss and runtime.\"\"\"\n",
    "\n",
    "\t\t\tdef begin(self):\n",
    "\t\t\t\tself._step = -1\n",
    "\t\t\t\tself._start_time = time.time()\n",
    "\n",
    "\t\t\tdef before_run(self, run_context):\n",
    "\t\t\t\tself._step += 1\n",
    "\t\t\t\treturn tf.train.SessionRunArgs(loss)  # Asks for loss value.\n",
    "\n",
    "\t\t\tdef after_run(self, run_context, run_values):\n",
    "\t\t\t\tif self._step % FLAGS.log_frequency == 0:\n",
    "\t\t\t\t\tcurrent_time = time.time()\n",
    "\t\t\t\t\tduration = current_time - self._start_time\n",
    "\t\t\t\t\tself._start_time = current_time\n",
    "\n",
    "\t\t\t\t\tloss_value = run_values.results\n",
    "\t\t\t\t\texamples_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration\n",
    "\t\t\t\t\tsec_per_batch = float(duration / FLAGS.log_frequency)\n",
    "\n",
    "\t\t\t\t\tformat_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "\t\t\t\t\t\t\t\t  'sec/batch)')\n",
    "\t\t\t\t\tprint(format_str % (datetime.now(), self._step, loss_value,\n",
    "\t\t\t\t\t\t\t\t\t\texamples_per_sec, sec_per_batch))\n",
    "\n",
    "\t\twith tf.train.MonitoredTrainingSession(\n",
    "\t\t\tcheckpoint_dir=FLAGS.train_dir,\n",
    "\t\t\thooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),\n",
    "\t\t\t\t   tf.train.NanTensorHook(loss),\n",
    "\t\t\t\t   _LoggerHook()],\n",
    "\t\t\tconfig=tf.ConfigProto(\n",
    "\t\t\t\tlog_device_placement=FLAGS.log_device_placement)) as mon_sess:\n",
    "\t\t\twhile not mon_sess.should_stop():\n",
    "\t\t\t\tmon_sess.run(train_op)\n",
    "\n",
    "\n",
    "def main(argv=None):  # pylint: disable=unused-argument\n",
    "\tcifar10.maybe_download_and_extract()\n",
    "\tif tf.gfile.Exists(FLAGS.train_dir):\n",
    "\t\ttf.gfile.DeleteRecursively(FLAGS.train_dir)\n",
    "\ttf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "\ttrain()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\ttf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-20T23:33:59.427189Z",
     "start_time": "2017-04-20T23:33:59.250425Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Evaluation for CIFAR-10.\n",
    "\n",
    "Accuracy:\n",
    "cifar10_train.py achieves 83.0% accuracy after 100K steps (256 epochs\n",
    "of data) as judged by cifar10_eval.py.\n",
    "\n",
    "Speed:\n",
    "On a single Tesla K40, cifar10_train.py processes a single batch of 128 images\n",
    "in 0.25-0.35 sec (i.e. 350 - 600 images /sec). The model reaches ~86%\n",
    "accuracy after 100K steps in 8 hours of training time.\n",
    "\n",
    "Usage:\n",
    "Please see the tutorial and website for how to download the CIFAR-10\n",
    "data set, compile the program and train the model.\n",
    "\n",
    "http://tensorflow.org/tutorials/deep_cnn/\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import cifar10\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('eval_dir', '/tmp/cifar10_eval',\n",
    "\t\t\t\t\t\t   \"\"\"Directory where to write event logs.\"\"\")\n",
    "tf.app.flags.DEFINE_string('eval_data', 'test',\n",
    "\t\t\t\t\t\t   \"\"\"Either 'test' or 'train_eval'.\"\"\")\n",
    "tf.app.flags.DEFINE_string('checkpoint_dir', '/tmp/cifar10_train',\n",
    "\t\t\t\t\t\t   \"\"\"Directory where to read model checkpoints.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('eval_interval_secs', 60 * 5,\n",
    "\t\t\t\t\t\t\t\"\"\"How often to run the eval.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_examples', 10000,\n",
    "\t\t\t\t\t\t\t\"\"\"Number of examples to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('run_once', False,\n",
    "\t\t\t\t\t\t\t\"\"\"Whether to run eval only once.\"\"\")\n",
    "\n",
    "\n",
    "def eval_once(saver, summary_writer, top_k_op, summary_op):\n",
    "\t\"\"\"Run Eval once.\n",
    "\n",
    "\tArgs:\n",
    "\t\t  saver: Saver.\n",
    "\t\t  summary_writer: Summary writer.\n",
    "\t\t  top_k_op: Top K op.\n",
    "\t\t  summary_op: Summary op.\n",
    "\t\"\"\"\n",
    "\twith tf.Session() as sess:\n",
    "\t\tckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n",
    "\t\tif ckpt and ckpt.model_checkpoint_path:\n",
    "\t\t\t# Restores from checkpoint\n",
    "\t\t\tsaver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\t\t\t# Assuming model_checkpoint_path looks something like:\n",
    "\t\t\t#   /my-favorite-path/cifar10_train/model.ckpt-0,\n",
    "\t\t\t# extract global_step from it.\n",
    "\t\t\tglobal_step = ckpt.model_checkpoint_path.split(\n",
    "\t\t\t\t'/')[-1].split('-')[-1]\n",
    "\t\telse:\n",
    "\t\t\tprint('No checkpoint file found')\n",
    "\t\t\treturn\n",
    "\n",
    "\t\t# Start the queue runners.\n",
    "\t\tcoord = tf.train.Coordinator()\n",
    "\t\ttry:\n",
    "\t\t\tthreads = []\n",
    "\t\t\tfor qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n",
    "\t\t\t\tthreads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t start=True))\n",
    "\n",
    "\t\t\tnum_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size))\n",
    "\t\t\ttrue_count = 0  # Counts the number of correct predictions.\n",
    "\t\t\ttotal_sample_count = num_iter * FLAGS.batch_size\n",
    "\t\t\tstep = 0\n",
    "\t\t\twhile step < num_iter and not coord.should_stop():\n",
    "\t\t\t\tpredictions = sess.run([top_k_op])\n",
    "\t\t\t\ttrue_count += np.sum(predictions)\n",
    "\t\t\t\tstep += 1\n",
    "\n",
    "\t\t\t# Compute precision @ 1.\n",
    "\t\t\tprecision = true_count / total_sample_count\n",
    "\t\t\tprint('%s: precision @ 1 = %.3f' % (datetime.now(), precision))\n",
    "\n",
    "\t\t\tsummary = tf.Summary()\n",
    "\t\t\tsummary.ParseFromString(sess.run(summary_op))\n",
    "\t\t\tsummary.value.add(tag='Precision @ 1', simple_value=precision)\n",
    "\t\t\tsummary_writer.add_summary(summary, global_step)\n",
    "\t\texcept Exception as e:  # pylint: disable=broad-except\n",
    "\t\t\tcoord.request_stop(e)\n",
    "\n",
    "\t\tcoord.request_stop()\n",
    "\t\tcoord.join(threads, stop_grace_period_secs=10)\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "\t\"\"\"Eval CIFAR-10 for a number of steps.\"\"\"\n",
    "\twith tf.Graph().as_default() as g:\n",
    "\t\t# Get images and labels for CIFAR-10.\n",
    "\t\teval_data = FLAGS.eval_data == 'test'\n",
    "\t\timages, labels = cifar10.inputs(eval_data=eval_data)\n",
    "\n",
    "\t\t# Build a Graph that computes the logits predictions from the\n",
    "\t\t# inference model.\n",
    "\t\tlogits = cifar10.inference(images)\n",
    "\n",
    "\t\t# Calculate predictions.\n",
    "\t\ttop_k_op = tf.nn.in_top_k(logits, labels, 1)\n",
    "\n",
    "\t\t# Restore the moving average version of the learned variables for eval.\n",
    "\t\tvariable_averages = tf.train.ExponentialMovingAverage(\n",
    "\t\t\tcifar10.MOVING_AVERAGE_DECAY)\n",
    "\t\tvariables_to_restore = variable_averages.variables_to_restore()\n",
    "\t\tsaver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "\t\t# Build the summary operation based on the TF collection of Summaries.\n",
    "\t\tsummary_op = tf.summary.merge_all()\n",
    "\n",
    "\t\tsummary_writer = tf.summary.FileWriter(FLAGS.eval_dir, g)\n",
    "\n",
    "\t\twhile True:\n",
    "\t\t\teval_once(saver, summary_writer, top_k_op, summary_op)\n",
    "\t\t\tif FLAGS.run_once:\n",
    "\t\t\t\tbreak\n",
    "\t\t\ttime.sleep(FLAGS.eval_interval_secs)\n",
    "\n",
    "\n",
    "def main(argv=None):  # pylint: disable=unused-argument\n",
    "\tcifar10.maybe_download_and_extract()\n",
    "\tif tf.gfile.Exists(FLAGS.eval_dir):\n",
    "\t\ttf.gfile.DeleteRecursively(FLAGS.eval_dir)\n",
    "\ttf.gfile.MakeDirs(FLAGS.eval_dir)\n",
    "\tevaluate()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\ttf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
